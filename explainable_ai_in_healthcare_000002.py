# -*- coding: utf-8 -*-
"""Explainable_AI_in_healthcare-000002.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1nHpyulpUa6yQfsKQFjdvkzPtMTTQKzR7
"""

# Install necessary libraries
!pip install lime scikit-learn

# Import necessary libraries
import numpy as np
import pandas as pd
import shap
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from lime.lime_text import LimeTextExplainer
from sklearn.pipeline import make_pipeline

# Sample data: Sentences and corresponding sentiment labels (1: Positive, 0: Negative)
data = {
    "text": [
        "The doctor was very helpful and attentive during my visit.",
        "I had a terrible experience with the hospital staff.",
        "The treatment was effective and I feel much better now.",
        "The waiting time was too long, and the nurses were rude.",
        "Great service! The team was very professional.",
        "I wouldn't recommend this clinic due to poor service."
    ],
    "sentiment": [1, 0, 1, 0, 1, 0]  # 1: Positive, 0: Negative
}

# Convert data into a DataFrame
df = pd.DataFrame(data)

# Split the data into train and test sets
X_train, X_test, y_train, y_test = train_test_split(df['text'], df['sentiment'], test_size=0.2, random_state=42)

# Create a TF-IDF vectorizer and Logistic Regression classifier pipeline
vectorizer = TfidfVectorizer()
classifier = LogisticRegression()

# Combine vectorizer and classifier in a pipeline
model_pipeline = make_pipeline(vectorizer, classifier)

# Train the model
model_pipeline.fit(X_train, y_train)

# 1. Local Interpretability using LIME
explainer_lime = LimeTextExplainer(class_names=["Negative", "Positive"])

# Predict the sentiment for a sample text and explain using LIME
sample_text = X_test.iloc[0]
exp = explainer_lime.explain_instance(sample_text, model_pipeline.predict_proba, num_features=5)

# Display the explanation
print(f"Prediction: {model_pipeline.predict([sample_text])[0]}")
print("Explanation:")
exp.show_in_notebook(text=sample_text)

# 2. Global Interpretability using SHAP
# Fit SHAP explainer (global interpretability)
explainer_shap = shap.Explainer(model_pipeline.named_steps['logisticregression'], vectorizer.transform(X_train))
shap_values = explainer_shap(vectorizer.transform(X_train))

# Plot the global feature importance using SHAP
shap.summary_plot(shap_values, vectorizer.transform(X_train), feature_names=vectorizer.get_feature_names_out())

#3. Counterfactual Explanation
def generate_counterfactual(text, word_to_change, new_word):
    """Modify the text by replacing a word and predict the new sentiment."""
    modified_text = text.replace(word_to_change, new_word)
    new_prediction = model_pipeline.predict([modified_text])[0]
    new_prob = model_pipeline.predict_proba([modified_text])[0]
    return modified_text, new_prediction, new_prob

# Example: Generate a counterfactual by changing "terrible" to "great"
cf_text, cf_prediction, cf_prob = generate_counterfactual(sample_text, "terrible", "great")

print(f"\nOriginal Sentence: '{sample_text}'")
print(f"Counterfactual Sentence: '{cf_text}'")
print(f"New Prediction: {cf_prediction} with probabilities {cf_prob}")

# 4. SHAP Visualization for the counterfactual sentence
cf_shap_values = explainer_shap(vectorizer.transform([cf_text]))
# SHAP Waterfall plot for counterfactual explanation
shap.waterfall_plot(cf_shap_values[0])

