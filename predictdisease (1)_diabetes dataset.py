# -*- coding: utf-8 -*-
"""predictdisease.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1jEGN5dEqNfHECbq5CRMeZO8N9sOtG-rg
"""

# -*- coding: utf-8 -*-
"""HAIML EXP 4.ipynb

This notebook performs a diabetes prediction analysis using machine learning techniques.
"""

# Import necessary libraries
import pandas as pd  # For data manipulation and analysis
import numpy as np  # For numerical operations
import seaborn as sns  # For data visualization
import matplotlib.pyplot as plt  # For plotting graphs
from sklearn.metrics import accuracy_score  # For evaluating model performance
from sklearn.model_selection import train_test_split  # For splitting the dataset into training and test sets
from sklearn.linear_model import LogisticRegression  # For logistic regression model
from sklearn.ensemble import RandomForestClassifier  # For random forest model

# Load the dataset
df = pd.read_csv("diabetes.csv")  # Read the CSV file into a DataFrame

# Display the first few rows of the DataFrame
print(df.head())  # Preview the data

# Get a statistical summary of the dataset
print(df.describe())  # Descriptive statistics

# Get information about the DataFrame
print(df.info())  # Column types and non-null counts

# Check for missing values
print("Missing values in the dataset:", df.isnull().values.any())  # Returns True if any missing values exist



# Outlier removal using the Interquartile Range (IQR) method
Q1 = df.quantile(0.25)  # First quartile (25th percentile)
Q3 = df.quantile(0.75)  # Third quartile (75th percentile)
IQR = Q3 - Q1  # Interquartile range

# Print the quartile values and IQR for reference
print("---Q1--- \n", Q1)
print("\n---Q3--- \n", Q3)
print("\n---IQR---\n", IQR)

# Remove outliers from the DataFrame based on IQR
df_out = df[~((df < (Q1 - 1.5 * IQR)) | (df > (Q3 + 1.5 * IQR))).any(axis=1)]  # Filter out outliers
print(f"Original shape: {df.shape}, After outlier removal shape: {df_out.shape}")  # Print shapes before and after

# Split the data into features (X) and target (y)
X = df_out.drop(columns=['Outcome'])  # Features: all columns except 'Outcome'
y = df_out['Outcome']  # Target: 'Outcome' column

# Split the data into training and testing sets (80% train, 20% test)
train_X, test_X, train_y, test_y = train_test_split(X, y, test_size=0.2, random_state=42)  # Set random state for reproducibility

# Print the shapes of the training and testing sets
print("Training set shape:", train_X.shape, train_y.shape)
print("Testing set shape:", test_X.shape, test_y.shape)

# Logistic Regression Model
clf_lr = LogisticRegression(solver='lbfgs', max_iter=1000)  # Initialize the logistic regression model
clf_lr.fit(train_X, train_y)  # Train the model on training data
y_pred_lr = clf_lr.predict(test_X)  # Make predictions on the test data

# Evaluate the logistic regression model's accuracy
accuracy_lr = accuracy_score(test_y, y_pred_lr)  # Calculate accuracy
print(f"Logistic Regression Predictions: {y_pred_lr}")  # Print predicted outcomes
print(f"Logistic Regression Accuracy: {accuracy_lr:.4f}")  # Print accuracy

# Random Forest Model
clf_rf = RandomForestClassifier(random_state=42)  # Initialize the random forest model
clf_rf.fit(train_X, train_y)  # Train the model on training data

# Make predictions on the test data
y_pred_rf = clf_rf.predict(test_X)  # Predict outcomes for the test data
print(f"Random Forest Predictions: {y_pred_rf}")  # Print predicted outcomes

# Evaluate the random forest model's accuracy
accuracy_rf = accuracy_score(test_y, y_pred_rf)  # Calculate accuracy
print(f"Random Forest Accuracy: {accuracy_rf:.4f}")  # Print accuracy